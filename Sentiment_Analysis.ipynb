{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rxbWk7_kyZ8"
   },
   "source": [
    "# Sentiment Analysis on Youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BEEVNSDxUjd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69b41geEwiiK"
   },
   "source": [
    "### Load the tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVygwwx-wN6Y",
    "outputId": "fcd96f96-15ec-4b6a-926d-59632b55bdf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join('..', 'data', 'twitter_sentiment')\n",
    "output_dir = os.path.join('..', 'Outputs')\n",
    "model_dir = os.path.join('..', 'Models')\n",
    "train_file_name = 'train.csv'\n",
    "dataset = pd.read_csv(os.path.join(data_dir, train_file_name), encoding = \"ISO-8859-1\", header = None, names = ['target','id','date','flag','user', 'text',])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "QbuXYmOp1VCb",
    "outputId": "5e995775-e4a1-4150-c328-a254f0fdb1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Number of Training examples (1600000, 6)\n"
     ]
    }
   ],
   "source": [
    "dict_target = {'negative':0, 'positive':1}\n",
    "print('Training data')\n",
    "print('Number of Training examples', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIgIECw88nAr"
   },
   "source": [
    "We will extract out only the required columns, that is target and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDaphrVI8nAw",
    "outputId": "8031c2b6-18f5-4184-ed02-e91c695cc6a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.drop(columns = ['id','date','flag','user'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89PvOzaz8Exa"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. We need to remove stop words, links, usernames  and a lot of other trash from the tweets as they don't convey any sentiment.\n",
    "    \n",
    "    So let us write a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "zcUvyON350hk",
    "outputId": "e02ea440-4c24-42cd-e5bf-6f46b93359d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sachin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sachin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sachin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNRHbWq6sqE_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/miniconda3/envs/acnn/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>switchfoot httptwitpiccomyzl awww thats bummer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>kenichan dived many time ball managed save res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass behaving im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  switchfoot httptwitpiccomyzl awww thats bummer...\n",
       "1       0  upset cant update facebook texting might cry r...\n",
       "2       0  kenichan dived many time ball managed save res...\n",
       "3       0                    whole body feel itchy like fire\n",
       "4       0           nationwideclass behaving im mad cant see"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#extract the most common words in english language\n",
    "stop_words = stopwords.words(\"english\")\n",
    "#intialise lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # remove emoji\n",
    "    #text = emoji.get_emoji_regexp().sub(r'', text.decode('utf8'))\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove punctuarion\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #remove usernames\n",
    "    text = re.sub(r'@[^\\s]+','', text)\n",
    "    \n",
    "    #remove links\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text) \n",
    "    \n",
    "    # heeeelllloooo => heelloo\n",
    "    text = re.sub(r\"(.)\\1{4,}\", r\"\\1\"*4, text)\n",
    "    \n",
    "    #remove whitespaces from beginning and end\n",
    "    text = text.strip()\n",
    "    \n",
    "    #tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens = []\n",
    "    \n",
    "    #remove stop words\n",
    "    for token in word_tokens:\n",
    "        if token not in stop_words:\n",
    "            tokens.append(token)\n",
    " \n",
    "    #Lemmatization to reduce words to their base forms\n",
    "    lemm_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "    return \" \".join(lemm_tokens)\n",
    "\n",
    "\n",
    "try: # load processed and save dataset\n",
    "    prep_file = 'preprocessed_tweets.csv'\n",
    "    dataset = pd.read_csv(os.path.join(data_dir, prep_file) , index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print('Preprocessing the data. Will take few minutes!')\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: preprocess(x))\n",
    "    dataset.to_csv('../data/Tweet_data/preprocessed_tweets.csv') # save it for later\n",
    "    \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zc_-OO3pNgYE"
   },
   "source": [
    "### Word Embedding Matrix using Word2Vec algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXfoG9S_H_Qa"
   },
   "source": [
    "***Word Embeddings*** are vector representations that capture the context of the underlying words in relation to other words in the sentence. This transformation results in words having similar meaning being clustered closer together in the hyperplane and distinct words positioned further away in the hyperplane.\n",
    "\n",
    "And ***Word2Vec***  is a 2 layer neural network, whose input is a text corpus and it's output is a set of vectors, which form the ***Word Embedding matrix***.\n",
    "\n",
    "We can use ***pre-trained Word Embeddings*** as written in this keras [blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), which is a better option when our training data is relatively small.\n",
    "\n",
    "But Since we have a large amount of data with us, ***We will train our own Word Embeddings***, specific to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhPzVZg1Nupo"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SazYhp24F-Bw"
   },
   "outputs": [],
   "source": [
    "#We will create a list of words present in our text corpus\n",
    "Bigger_list = []\n",
    "for i in dataset['text']:\n",
    "    try:\n",
    "        li = list(i.split(\" \"))\n",
    "        Bigger_list.append(li)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m-zWE1bSFHY"
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "W2V_SIZE = 100    #Size of vector representing each word\n",
    "W2V_WINDOW = 7    \n",
    "W2V_EPOCH = 32    \n",
    "W2V_MIN_COUNT = 10  #Minimum number of times, the word should appear in text corpus\n",
    "                    #for it to be included in vocabulary\n",
    "                    #keeping 10, helps to avoid usernames present in tweets\n",
    "\n",
    "try: # load already saved model\n",
    "    model_file = 'model.w2v'\n",
    "    w2v_model = Word2Vec.load(os.path.join(output_dir, model_file))\n",
    "except FileNotFoundError:\n",
    "    print('Training the Word2Vec model. Will take few mins!')\n",
    "    w2v_model = Word2Vec(Bigger_list, size = W2V_SIZE, window = W2V_WINDOW, min_count = W2V_MIN_COUNT, workers = 8)\n",
    "    w2v_model.save(\"model.w2v\") #save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z90k9rIdFPJD"
   },
   "source": [
    "Now, Let's create a dictionary mapping each word in Vocabulary to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vAlQf3lmUNsU",
    "outputId": "4e5acc94-7fd5-44c8-bd65-2d6f1d218361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary : 41207\n"
     ]
    }
   ],
   "source": [
    "#let's check out the vocabulary\n",
    "vocab = list(w2v_model.wv.vocab)\n",
    "print('Length of Vocabulary :',len(vocab))\n",
    "\n",
    "#and create the dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab, 1): \n",
    "    word_index[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0yJz4fHGT73"
   },
   "source": [
    "Let us analyze that our Word2Vec model if it learned correct relation in between the words present in text corpus. We can do that by finding similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "MJBkFCeMUwlC",
    "outputId": "05ece510-7839-41db-ee99-c767506948a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to great\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.8335270881652832),\n",
       " ('wonderful', 0.7792121171951294),\n",
       " ('good', 0.7575284242630005),\n",
       " ('fabulous', 0.7514442205429077),\n",
       " ('awesome', 0.742106556892395)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check similarity\n",
    "test_word = \"great\"\n",
    "print('Top 5 words similar to', test_word)\n",
    "w2v_model.wv.most_similar(test_word, topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIhVnb3_HXJG"
   },
   "source": [
    "Now we will club all the vectors together and form a ***Word Embedding Matrix*** which will be passed into the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3ISRc-U8V7pg",
    "outputId": "8899e721-53d9-4c19-e2c1-25fca8bcbbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix : (41208, 100)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)+1   #one extra row for \"out of vocabulary words\"\n",
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE)) #initialising the matrix with zeros\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]  #adding vector to the matrix\n",
    "\n",
    "print('Shape of embedding matrix :', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N0y7kSgSZJD"
   },
   "source": [
    "#### Preparing the input to the Nerual Network.\n",
    "\n",
    "We will transform the tweets to their integer form using the word_index dictionary. And, since not all the tweets are of same length, we will pad the shorter tweets with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfh-dPYL8nDl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest tweet is 56\n",
      "upset cant update facebook texting might cry result school today also blah \n",
      " mapped to \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20.\n",
      " 21. 22.]\n"
     ]
    }
   ],
   "source": [
    "def text_to_int(df, column, word_index, max_len):\n",
    "    '''\n",
    "        df : dataframe containing column \"text\"\n",
    "        word_index : Dictionary contiaing mapping from words to int\n",
    "        max_len : maximum length of each tweet\n",
    "    '''\n",
    "    X = np.zeros((df.shape[0], max_len))  #initialising the nd-array\n",
    "    \n",
    "    for i, tweet in enumerate(df[column]):\n",
    "        try:\n",
    "            words = list(tweet.split(\" \"))\n",
    "            j = 0\n",
    "            for word in reversed(words):\n",
    "                if word in word_index.keys():   #if present in our vocab\n",
    "                    X[i, max_len-1-j] = word_index[word]\n",
    "                    j += 1\n",
    "        except:\n",
    "            pass\n",
    "    return X\n",
    "\n",
    "#finding the longest tweet\n",
    "max_len = 0\n",
    "for list_ in Bigger_list:\n",
    "    if len(list_)>max_len:\n",
    "        max_len = len(list_)\n",
    "\n",
    "print('Length of longest tweet is',max_len)\n",
    "\n",
    "#converting train_data tweets to integer\n",
    "X_train = text_to_int(dataset, 'text', word_index, max_len)\n",
    "print(dataset.text[1], '\\n mapped to \\n', X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5dMXLoU8nDR"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "MTwdAko18nDZ",
    "outputId": "79520c03-b163-45e7-acfc-63558e3e8f16"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# personal scripts\n",
    "import rnnmodel\n",
    "#importlib.reload(rnnmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kso59EkF9H1g"
   },
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGMHOSf98nEX"
   },
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(X_train.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        sample = [x_train[idx], target[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFYCiPnQ8nEe"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    # Start training\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target) # Cause we have 3 classes\n",
    "        loss.backward()\n",
    "        pred = torch.round(output)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    # print accuracy\n",
    "    print('\\nTraining Accuracy: {}/{} ({:.4f}%) \\n'.format(correct, len(train_loader.dataset),\n",
    "                                                    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LHm18Ib9SPV"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7yNW9VR8nED"
   },
   "outputs": [],
   "source": [
    "# Converting to torch tensors\n",
    "x_train = torch.from_numpy(X_train).to(device, torch.int64)\n",
    "embed_matrix = torch.from_numpy(embedding_matrix).to(device)\n",
    "target = torch.from_numpy(dataset.target.values/4).to(device, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZX_Tvb68nEk",
    "outputId": "983bce12-06d7-444d-f67c-bdf698870318",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin/miniconda3/envs/acnn/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1600000 (0%)]\tLoss: 0.688409\n",
      "Train Epoch: 0 [64000/1600000 (4%)]\tLoss: 0.687709\n",
      "Train Epoch: 0 [128000/1600000 (8%)]\tLoss: 0.688220\n",
      "Train Epoch: 0 [192000/1600000 (12%)]\tLoss: 0.665353\n",
      "Train Epoch: 0 [256000/1600000 (16%)]\tLoss: 0.664088\n",
      "Train Epoch: 0 [320000/1600000 (20%)]\tLoss: 0.570513\n",
      "Train Epoch: 0 [384000/1600000 (24%)]\tLoss: 0.515118\n",
      "Train Epoch: 0 [448000/1600000 (28%)]\tLoss: 0.449832\n",
      "Train Epoch: 0 [512000/1600000 (32%)]\tLoss: 0.491683\n",
      "Train Epoch: 0 [576000/1600000 (36%)]\tLoss: 0.476641\n",
      "Train Epoch: 0 [640000/1600000 (40%)]\tLoss: 0.412632\n",
      "Train Epoch: 0 [704000/1600000 (44%)]\tLoss: 0.586829\n",
      "Train Epoch: 0 [768000/1600000 (48%)]\tLoss: 0.523383\n",
      "Train Epoch: 0 [832000/1600000 (52%)]\tLoss: 0.518665\n",
      "Train Epoch: 0 [896000/1600000 (56%)]\tLoss: 0.547207\n",
      "Train Epoch: 0 [960000/1600000 (60%)]\tLoss: 0.526090\n",
      "Train Epoch: 0 [1024000/1600000 (64%)]\tLoss: 0.444460\n",
      "Train Epoch: 0 [1088000/1600000 (68%)]\tLoss: 0.502224\n",
      "Train Epoch: 0 [1152000/1600000 (72%)]\tLoss: 0.538651\n",
      "Train Epoch: 0 [1216000/1600000 (76%)]\tLoss: 0.421524\n",
      "Train Epoch: 0 [1280000/1600000 (80%)]\tLoss: 0.529389\n",
      "Train Epoch: 0 [1344000/1600000 (84%)]\tLoss: 0.482893\n",
      "Train Epoch: 0 [1408000/1600000 (88%)]\tLoss: 0.603452\n",
      "Train Epoch: 0 [1472000/1600000 (92%)]\tLoss: 0.501004\n",
      "Train Epoch: 0 [1536000/1600000 (96%)]\tLoss: 0.484932\n",
      "\n",
      "Training Accuracy: 1153754/1600000 (72.1096%) \n",
      "\n",
      "Train Epoch: 1 [0/1600000 (0%)]\tLoss: 0.601749\n",
      "Train Epoch: 1 [64000/1600000 (4%)]\tLoss: 0.391246\n",
      "Train Epoch: 1 [128000/1600000 (8%)]\tLoss: 0.447784\n",
      "Train Epoch: 1 [192000/1600000 (12%)]\tLoss: 0.324795\n",
      "Train Epoch: 1 [256000/1600000 (16%)]\tLoss: 0.428233\n",
      "Train Epoch: 1 [320000/1600000 (20%)]\tLoss: 0.525932\n",
      "Train Epoch: 1 [384000/1600000 (24%)]\tLoss: 0.385150\n",
      "Train Epoch: 1 [448000/1600000 (28%)]\tLoss: 0.415218\n",
      "Train Epoch: 1 [512000/1600000 (32%)]\tLoss: 0.524446\n",
      "Train Epoch: 1 [576000/1600000 (36%)]\tLoss: 0.475893\n",
      "Train Epoch: 1 [640000/1600000 (40%)]\tLoss: 0.521742\n",
      "Train Epoch: 1 [704000/1600000 (44%)]\tLoss: 0.512213\n",
      "Train Epoch: 1 [768000/1600000 (48%)]\tLoss: 0.464636\n",
      "Train Epoch: 1 [832000/1600000 (52%)]\tLoss: 0.527911\n",
      "Train Epoch: 1 [896000/1600000 (56%)]\tLoss: 0.581773\n",
      "Train Epoch: 1 [960000/1600000 (60%)]\tLoss: 0.526542\n",
      "Train Epoch: 1 [1024000/1600000 (64%)]\tLoss: 0.354603\n",
      "Train Epoch: 1 [1088000/1600000 (68%)]\tLoss: 0.541729\n",
      "Train Epoch: 1 [1152000/1600000 (72%)]\tLoss: 0.529944\n",
      "Train Epoch: 1 [1216000/1600000 (76%)]\tLoss: 0.462810\n",
      "Train Epoch: 1 [1280000/1600000 (80%)]\tLoss: 0.511991\n",
      "Train Epoch: 1 [1344000/1600000 (84%)]\tLoss: 0.446074\n",
      "Train Epoch: 1 [1408000/1600000 (88%)]\tLoss: 0.455481\n",
      "Train Epoch: 1 [1472000/1600000 (92%)]\tLoss: 0.406363\n",
      "Train Epoch: 1 [1536000/1600000 (96%)]\tLoss: 0.449580\n",
      "\n",
      "Training Accuracy: 1231483/1600000 (76.9677%) \n",
      "\n",
      "Train Epoch: 2 [0/1600000 (0%)]\tLoss: 0.654527\n",
      "Train Epoch: 2 [64000/1600000 (4%)]\tLoss: 0.408872\n",
      "Train Epoch: 2 [128000/1600000 (8%)]\tLoss: 0.392840\n",
      "Train Epoch: 2 [192000/1600000 (12%)]\tLoss: 0.501691\n",
      "Train Epoch: 2 [256000/1600000 (16%)]\tLoss: 0.343435\n",
      "Train Epoch: 2 [320000/1600000 (20%)]\tLoss: 0.328616\n",
      "Train Epoch: 2 [384000/1600000 (24%)]\tLoss: 0.370073\n",
      "Train Epoch: 2 [448000/1600000 (28%)]\tLoss: 0.436216\n",
      "Train Epoch: 2 [512000/1600000 (32%)]\tLoss: 0.487016\n",
      "Train Epoch: 2 [576000/1600000 (36%)]\tLoss: 0.526733\n",
      "Train Epoch: 2 [640000/1600000 (40%)]\tLoss: 0.365088\n",
      "Train Epoch: 2 [704000/1600000 (44%)]\tLoss: 0.397634\n",
      "Train Epoch: 2 [768000/1600000 (48%)]\tLoss: 0.410445\n",
      "Train Epoch: 2 [832000/1600000 (52%)]\tLoss: 0.448770\n",
      "Train Epoch: 2 [896000/1600000 (56%)]\tLoss: 0.500144\n",
      "Train Epoch: 2 [960000/1600000 (60%)]\tLoss: 0.581264\n",
      "Train Epoch: 2 [1024000/1600000 (64%)]\tLoss: 0.421180\n",
      "Train Epoch: 2 [1088000/1600000 (68%)]\tLoss: 0.574502\n",
      "Train Epoch: 2 [1152000/1600000 (72%)]\tLoss: 0.391224\n",
      "Train Epoch: 2 [1216000/1600000 (76%)]\tLoss: 0.475259\n",
      "Train Epoch: 2 [1280000/1600000 (80%)]\tLoss: 0.557640\n",
      "Train Epoch: 2 [1344000/1600000 (84%)]\tLoss: 0.374572\n",
      "Train Epoch: 2 [1408000/1600000 (88%)]\tLoss: 0.462525\n",
      "Train Epoch: 2 [1472000/1600000 (92%)]\tLoss: 0.466622\n",
      "Train Epoch: 2 [1536000/1600000 (96%)]\tLoss: 0.387220\n",
      "\n",
      "Training Accuracy: 1244682/1600000 (77.7926%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "load = False\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "epochs = 3\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "\n",
    "# Data loader\n",
    "train_dataset = tweetDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# model    \n",
    "model = rnnmodel.RNNModel(embed_matrix, hidden_size, num_layers).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=1000)\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, f'model_{str(datetime.now())}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sOBdX7cwATe"
   },
   "source": [
    "## Testing on Youtube Comments data (for product - CyberTruck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deFTXKcz8nFB",
    "outputId": "2945188f-8222-439d-b6c9-e9ef7f9aa4a5"
   },
   "outputs": [],
   "source": [
    "import youtube_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZYpS96f8nFK",
    "outputId": "6b4b5a3b-271d-487c-ed2b-6ee43e9bf857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a keyword(related to product): cybertruck\n",
      "Enter max_pages: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>SwvDOdBHYBw</td>\n",
       "      <td>WATCH LIVE! Elon Musk presents the new Tesla C...</td>\n",
       "      <td>They released this robotic version of Elon Mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>SwvDOdBHYBw</td>\n",
       "      <td>WATCH LIVE! Elon Musk presents the new Tesla C...</td>\n",
       "      <td>That’s literally a halo warthog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SwvDOdBHYBw</td>\n",
       "      <td>WATCH LIVE! Elon Musk presents the new Tesla C...</td>\n",
       "      <td>When you see the car which you used to draw in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>SwvDOdBHYBw</td>\n",
       "      <td>WATCH LIVE! Elon Musk presents the new Tesla C...</td>\n",
       "      <td>Now hit it with a sledgehammer that's not rubb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SwvDOdBHYBw</td>\n",
       "      <td>WATCH LIVE! Elon Musk presents the new Tesla C...</td>\n",
       "      <td>Him trying to speak gives me anxiety.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  SwvDOdBHYBw  WATCH LIVE! Elon Musk presents the new Tesla C...   \n",
       "1  SwvDOdBHYBw  WATCH LIVE! Elon Musk presents the new Tesla C...   \n",
       "2  SwvDOdBHYBw  WATCH LIVE! Elon Musk presents the new Tesla C...   \n",
       "3  SwvDOdBHYBw  WATCH LIVE! Elon Musk presents the new Tesla C...   \n",
       "4  SwvDOdBHYBw  WATCH LIVE! Elon Musk presents the new Tesla C...   \n",
       "\n",
       "                                             comment  \n",
       "0  They released this robotic version of Elon Mus...  \n",
       "1                    That’s literally a halo warthog  \n",
       "2  When you see the car which you used to draw in...  \n",
       "3  Now hit it with a sledgehammer that's not rubb...  \n",
       "4              Him trying to speak gives me anxiety.  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n",
    "service = youtube_scraper.get_authenticated_service()\n",
    "keyword = input('Enter a keyword(related to product): ')\n",
    "max_pages = int(input('Enter max_pages: '))\n",
    "yt_data = youtube_scraper.extract_comments_by_video_keyword(service, max_pages, q=keyword, \n",
    "                                                            part='id,snippet', eventType='completed', \n",
    "                                                            type='video')\n",
    "yt_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcNQSyDV_DhK"
   },
   "source": [
    "### Preprocess youtube comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1lJFHQJ8nFW"
   },
   "outputs": [],
   "source": [
    "yt_pro_data = yt_data.copy()\n",
    "yt_pro_data.comment = yt_pro_data.comment.apply(lambda x:preprocess(x))\n",
    "\n",
    "yt_cmts = text_to_int(yt_pro_data, 'comment', word_index, max_len)\n",
    "num_cmts = len(yt_cmts)  # feeding all comments together\n",
    "\n",
    "# Preparing input to the model\n",
    "h = torch.zeros((num_layers, num_cmts, hidden_size)).to(device)\n",
    "c = torch.zeros((num_layers, num_cmts, hidden_size)).to(device)\n",
    "cmts_data = torch.tensor(yt_cmts).to(device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkhgK5e_ALUw"
   },
   "source": [
    "### Sentiment classification of youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6Y2BbdG8nFb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "They released this robotic version of Elon Musk before the A.I. was ready. --> positive\n",
      "----------\n",
      "That’s literally a halo warthog --> neutral\n",
      "----------\n",
      "When you see the car which you used to draw in the first grade.. --> neutral\n",
      "----------\n",
      "Now hit it with a sledgehammer that's not rubber on the outside --> neutral\n",
      "----------\n",
      "Him trying to speak gives me anxiety. --> negative\n",
      "----------\n",
      "great mind, less great speeches. I still like him :) --> positive\n",
      "----------\n",
      "I appreciate elon musk. I feel that he is ahead of the pack in spearheading electric technologies --> positive\n",
      "----------\n",
      "Your going to see the roads full of Tesla’s soon. Idk the cons but I’d like to see the reviews. I can’t imagine what will be going on 100yrs from now. Good job --> neutral\n",
      "----------\n",
      "Welcome back drawing I made about 6 years ago --> positive\n",
      "----------\n",
      "The man is brilliant, he hires the best engineers and challenges them with vision --> positive\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output = model(cmts_data)\n",
    "\n",
    "pred = []\n",
    "for i in range(num_cmts):\n",
    "    if output[i]<0.4:\n",
    "        pred.append('negative')\n",
    "    elif output[i]>0.6:\n",
    "        pred.append('positive')\n",
    "    else:\n",
    "        pred.append('neutral')\n",
    "        \n",
    "# save sentiment to dataframe\n",
    "sent_data = pd.DataFrame(pred, columns=['sentiment'])\n",
    "yt_cmts = yt_data.copy()\n",
    "#yt_sent_data = yt_cmts.join(sent_data)\n",
    "\n",
    "# Print comments and their sentiment\n",
    "num_print = 10\n",
    "for i in range(num_print):\n",
    "    print('-'*10)\n",
    "    print(yt_sent_data.comment[i], '-->',yt_sent_data.sentiment[i])\n",
    "\n",
    "# saving output\n",
    "pd.to_csv(yt_sent_data, os.path.join(output_dir, f'{keyword}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iolp0LjLE8ym"
   },
   "source": [
    "#### Thank u"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_Analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
