{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rxbWk7_kyZ8"
   },
   "source": [
    "## Sentiment Analysis on Youtube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BEEVNSDxUjd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d81037f7a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69b41geEwiiK"
   },
   "source": [
    "### Load the tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVygwwx-wN6Y"
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/twitter_sentiment'\n",
    "output_dir = '../Outputs'\n",
    "train_file_name = 'train.csv'\n",
    "dataset = pd.read_csv(os.path.join(data_dir, train_file_name), encoding = \"ISO-8859-1\", header = None, names = ['target','id','date','flag','user', 'text',])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "QbuXYmOp1VCb",
    "outputId": "5e995775-e4a1-4150-c328-a254f0fdb1a8"
   },
   "outputs": [],
   "source": [
    "dict_target = {'negative':0, 'neutral':2, 'positive':4}\n",
    "print('Training data')\n",
    "print('Number of Training examples', dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract out only the required columns, that is target and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = ['id','date','flag','user'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89PvOzaz8Exa"
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "1. We need to remove stop words, links, usernames  and a lot of other trash from the tweets as they don't convey any sentiment.\n",
    "    \n",
    "    So let us write a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "zcUvyON350hk",
    "outputId": "e02ea440-4c24-42cd-e5bf-6f46b93359d2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNRHbWq6sqE_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#extract the most common words in english language\n",
    "stop_words = stopwords.words(\"english\")\n",
    "#intialise lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove punctuarion\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #remove usernames\n",
    "    text = re.sub(r'@[^\\s]+','', text)\n",
    "    \n",
    "    #remove links\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text) \n",
    "    \n",
    "    # heeeelllloooo => heelloo\n",
    "    text = re.sub(r\"(.)\\1{4,}\", r\"\\1\"*4, text)\n",
    "    \n",
    "    #remove whitespaces from beginning and end\n",
    "    text = text.strip()\n",
    "    \n",
    "    #tokenize\n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens = []\n",
    "    \n",
    "    #remove stop words\n",
    "    for token in word_tokens:\n",
    "        if token not in stop_words:\n",
    "            tokens.append(token)\n",
    " \n",
    "    #Lemmatization to reduce words to their base forms\n",
    "    lemm_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "    return \" \".join(lemm_tokens)\n",
    "\n",
    "\n",
    "try: # load processed and save dataset\n",
    "    prep_file = 'preprocessed_tweets.csv'\n",
    "    dataset = pd.read_csv(os.path.join(data_dir, prep_file) , index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print('Preprocessing the data. Will take few minutes!')\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: preprocess(x))\n",
    "    dataset.to_csv('../data/Tweet_data/preprocessed_tweets.csv') # save it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zc_-OO3pNgYE"
   },
   "source": [
    "### Word Embedding Matrix using Word2Vec algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXfoG9S_H_Qa"
   },
   "source": [
    "***Word Embeddings*** are vector representations that capture the context of the underlying words in relation to other words in the sentence. This transformation results in words having similar meaning being clustered closer together in the hyperplane and distinct words positioned further away in the hyperplane.\n",
    "\n",
    "And ***Word2Vec***  is a 2 layer neural network, whose input is a text corpus and it's output is a set of vectors, which form the ***Word Embedding matrix***.\n",
    "\n",
    "We can use ***pre-trained Word Embeddings*** as written in this keras [blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), which is a better option when our training data is relatively small.\n",
    "\n",
    "But Since we have a large amount of data with us, ***We will train our own Word Embeddings***, specific to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhPzVZg1Nupo"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SazYhp24F-Bw"
   },
   "outputs": [],
   "source": [
    "#We will create a list of words present in our text corpus\n",
    "Bigger_list = []\n",
    "for i in dataset['text']:\n",
    "    try:\n",
    "        li = list(i.split(\" \"))\n",
    "        Bigger_list.append(li)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "9m-zWE1bSFHY",
    "outputId": "7860d5d0-4d6e-45f4-df55-8087a85a8b75"
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "W2V_SIZE = 100    #Size of vector representing each word\n",
    "W2V_WINDOW = 7    \n",
    "W2V_EPOCH = 32    \n",
    "W2V_MIN_COUNT = 10  #Minimum number of times, the word should appear in text corpus\n",
    "                    #for it to be included in vocabulary\n",
    "                    #keeping 10, helps to avoid usernames present in tweets\n",
    "\n",
    "try: # load already saved model\n",
    "    model_file = 'model.w2v'\n",
    "    w2v_model = Word2Vec.load(os.path.join(output_dir, model_file))\n",
    "except FileNotFoundError:\n",
    "    print('Training the Word2Vec model. Will take few mins!')\n",
    "    w2v_model = Word2Vec(Bigger_list, size = W2V_SIZE, window = W2V_WINDOW, min_count = W2V_MIN_COUNT, workers = 8)\n",
    "    w2v_model.save(\"model.w2v\") #save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z90k9rIdFPJD"
   },
   "source": [
    "Now, Let's create a dictionary mapping each word in Vocabulary to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vAlQf3lmUNsU",
    "outputId": "4e5acc94-7fd5-44c8-bd65-2d6f1d218361"
   },
   "outputs": [],
   "source": [
    "#let's check out the vocabulary\n",
    "vocab = list(w2v_model.wv.vocab)\n",
    "print('Length of Vocabulary :',len(vocab))\n",
    "\n",
    "#and create the dictionary\n",
    "word_index = {}\n",
    "for i, word in enumerate(vocab, 1): \n",
    "    word_index[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0yJz4fHGT73"
   },
   "source": [
    "Let us analyze that our Word2Vec model if it learned correct relation in between the words present in text corpus. We can do that by finding similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "MJBkFCeMUwlC",
    "outputId": "05ece510-7839-41db-ee99-c767506948a3"
   },
   "outputs": [],
   "source": [
    "#let's check similarity\n",
    "test_word = \"great\"\n",
    "print('Top 5 words similar to', test_word)\n",
    "w2v_model.wv.most_similar(test_word, topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIhVnb3_HXJG"
   },
   "source": [
    "Now we will club all the vectors together and form a ***Word Embedding Matrix*** which will be passed into the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3ISRc-U8V7pg",
    "outputId": "8899e721-53d9-4c19-e2c1-25fca8bcbbb6"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)+1   #one extra row for \"out of vocabulary words\"\n",
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE)) #initialising the matrix with zeros\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]  #adding vector to the matrix\n",
    "\n",
    "print('Shape of embedding matrix :', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N0y7kSgSZJD"
   },
   "source": [
    "#### Preparing the input to the Nerual Network.\n",
    "\n",
    "We will transform the tweets to their integer form using the word_index dictionary. And, since not all the tweets are of same length, we will pad the shorter tweets with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, hidden_size,  num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = self.create_embedding_layer(embedding_matrix)\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=self.num_layers)\n",
    "        self.linear = nn.Linear(num_layers*hidden_size, 1)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        output, hidden = self.lstm(self.embedding(inp), hidden)\n",
    "        h, c = hidden\n",
    "        h = h.permute(1, 2, 0)\n",
    "        h = h.reshape(h.shape[0], -1)\n",
    "        y = self.linear(h)\n",
    "        return F.sigmoid(y), hidden\n",
    "\n",
    "    @staticmethod\n",
    "    def create_embedding_layer(embed_matrix, trainable=True):\n",
    "        num_embeddings, embedding_dim = embed_matrix.shape\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        emb_layer.load_state_dict({'weight':embed_matrix})\n",
    "        if trainable:\n",
    "            emb_layer.weight.requires_grad = True\n",
    "        return emb_layer, num_embeddings, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S4WqAJtlXKSD",
    "outputId": "6021e51f-2b4c-421a-af61-4af9077e238c"
   },
   "outputs": [],
   "source": [
    "def text_to_int(df, word_index, max_len):\n",
    "    '''\n",
    "        df : dataframe containing column \"text\"\n",
    "        word_index : Dictionary contiaing mapping from words to int\n",
    "        max_len : maximum length of each tweet\n",
    "    '''\n",
    "    X = np.zeros((df.shape[0], max_len))  #initialising the nd-array\n",
    "    \n",
    "    for i, tweet in enumerate(df.text):\n",
    "        try:\n",
    "            words = list(tweet.split(\" \"))\n",
    "            j = 0\n",
    "            for word in reversed(words):\n",
    "                if word in word_index.keys():   #if present in our vocab\n",
    "                    X[i, max_len-1-j] = word_index[word]\n",
    "                    j += 1\n",
    "        except:\n",
    "            pass\n",
    "    return X\n",
    "\n",
    "#finding the longest tweet\n",
    "max_len = 0\n",
    "for list_ in Bigger_list:\n",
    "    if len(list_)>max_len:\n",
    "        max_len = len(list_)\n",
    "\n",
    "print('Length of longest tweet is',max_len)\n",
    "\n",
    "#converting train_data tweets to integer\n",
    "X_train = text_to_int(dataset, word_index, max_len)\n",
    "print(dataset.text[1], '\\n mapped to \\n', X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to torch tensors\n",
    "x_train = torch.from_numpy(X_train).to(torch.int64)\n",
    "#embed_matrix = torch.from_numpy(embedding_matrix)\n",
    "target = torch.from_numpy(dataset.target.values).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(X_train.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        sample = [x_train[idx], target[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, h0, c0, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    h = h0.to(device)\n",
    "    c = c0.to(device)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, (h, c) = model(data, (h, c))\n",
    "        \n",
    "        # Detaching from graph\n",
    "        h.detach_()\n",
    "        c.detach_()\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 56\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.01\n",
    "epochs = 1\n",
    "\n",
    "train_dataset = tweetDataset()\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "    \n",
    "\n",
    "h0 = torch.rand((num_layers, batch_size, hidden_size)).to(device)\n",
    "c0 = torch.rand((num_layers, batch_size, hidden_size)).to(device)\n",
    "    \n",
    "model = RNNModel(embed_matrix, hidden_size, num_layers).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "epoch = 1\n",
    "train(model, h0, c0, device, train_loader, optimizer, epoch, log_interval=1000)\n",
    "#    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 200\n",
    "# num_layers = 2\n",
    "# embed_matrix = torch.from_numpy(embedding_matrix)\n",
    "# model = Model(embed_matrix, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.rand((50, 10))\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "model = RNNModel(embedding_matrix, hidden_size, num_layers)\n",
    "h = torch.rand((num_layers, 1, hidden_size))\n",
    "c = torch.rand((num_layers, 1, hidden_size))\n",
    "x = torch.arange(10).view((10, 1))\n",
    "output, (h, c) = model.forward(x, (h, c))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sOBdX7cwATe"
   },
   "source": [
    "### Testing on Youtube Comments data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment Analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
